{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import html\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import thaispellcheck\n",
    "from html.parser import HTMLParser\n",
    "from attacut import tokenize\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all pdf in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-ก.พ.-2566+Academic_100223_115125',\n",
       " '11-ธ.ค.-2555+Academic_111212_150945',\n",
       " '11-ธ.ค.-2555+Academic_111212_152818',\n",
       " '11-ธ.ค.-2555+Academic_111212_154616',\n",
       " '11-ธ.ค.-2555+Academic_111212_154855',\n",
       " '3-ก.พ.-2565+Academic_110821_091216',\n",
       " '3-ต.ค.-2566+Academic_031023_105516',\n",
       " '4-มิ.ย.-2556+Academic_040613_100424',\n",
       " '4-มิ.ย.-2556+Academic_040613_100623',\n",
       " '9-ส.ค.-2566+Academic_130623_110407']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_pdf = os.listdir('./pdf_example')\n",
    "list_of_pdf = [x for x in list_of_pdf if x.endswith('.pdf')]\n",
    "list_of_name = [list_of_pdf[num].split('.')[0]+'.'+list_of_pdf[num].split('.')[1]+'.'+list_of_pdf[num].split('.')[2] for num in range(len(list_of_pdf))]\n",
    "list_of_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to HTML with PDFBox ( java )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_example/10-ก.พ.-2566+Academic_100223_115125.pdf\n",
      "10-ก.พ.-2566+Academic_100223_115125\n",
      "pdf_example/11-ธ.ค.-2555+Academic_111212_150945.pdf\n",
      "11-ธ.ค.-2555+Academic_111212_150945\n",
      "pdf_example/11-ธ.ค.-2555+Academic_111212_152818.pdf\n",
      "11-ธ.ค.-2555+Academic_111212_152818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_example/11-ธ.ค.-2555+Academic_111212_154616.pdf\n",
      "11-ธ.ค.-2555+Academic_111212_154616\n",
      "pdf_example/11-ธ.ค.-2555+Academic_111212_154855.pdf\n",
      "11-ธ.ค.-2555+Academic_111212_154855\n",
      "pdf_example/3-ก.พ.-2565+Academic_110821_091216.pdf\n",
      "3-ก.พ.-2565+Academic_110821_091216\n",
      "pdf_example/3-ต.ค.-2566+Academic_031023_105516.pdf\n",
      "3-ต.ค.-2566+Academic_031023_105516\n",
      "pdf_example/4-มิ.ย.-2556+Academic_040613_100424.pdf\n",
      "4-มิ.ย.-2556+Academic_040613_100424\n",
      "pdf_example/4-มิ.ย.-2556+Academic_040613_100623.pdf\n",
      "4-มิ.ย.-2556+Academic_040613_100623\n",
      "pdf_example/9-ส.ค.-2566+Academic_130623_110407.pdf\n",
      "9-ส.ค.-2566+Academic_130623_110407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "for num in range(len(list_of_name)):\n",
    "    pdf_file = list_of_name[num]\n",
    "    path_to_pdf_file = 'pdf_example/'+pdf_file+'.pdf'\n",
    "    input_format = '--input='+path_to_pdf_file\n",
    "    print(path_to_pdf_file)\n",
    "    print(list_of_name[num])\n",
    "    output_path = \"./html_output/before_mapping\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    # คำสั่งที่ต้องการรัน\n",
    "    command = ['java', '-jar', 'pdfbox-app-3.0.2.jar', 'export:text', '-html', input_format, f'--output={output_path}/{list_of_name[num]}.html']\n",
    "\n",
    "    # รันคำสั่งและเก็บผลลัพธ์\n",
    "    result = subprocess.run(command, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping some Error Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_after_mapping, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m outputf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_after_mapping\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_of_name[num]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m inputf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_of_name[num]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(list_of_name[num])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m inputf:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_path' is not defined"
     ]
    }
   ],
   "source": [
    "pua = {\n",
    "    '63233': '&#3636;', # 0xf701 Sara I\n",
    "    '63234': '&#3637;', # 0xf702\n",
    "    '63235': '&#3638;', # 0xf703\n",
    "    '63236': '&#3639;', # 0xf704\n",
    "    '63237': '&#3656;', # 0xf705\n",
    "    '63238': '&#3657;', # 0xf706 Mai Tho (on Po Pla)\n",
    "    '63242': '&#3656;', # 0xf70a Mai Ek\n",
    "    '63243': '&#3657;', # 0xf70b Mai Tho\n",
    "    '63246': '&#3660;', # 0xf70e Thantakat\n",
    "    '63248': '&#3633;', # 0xf710  Mai Han Akhat (on Po Pla)\n",
    "    '63250': '&#3655;', # 0xf712 Mai Tai Khu (on Po Pla)\n",
    "    '63251': '&#3656;', # 0xf713\n",
    "    '63252': '&#3657;', # 0xf714\n",
    "    '63244': '&#3658;', # ๊\n",
    "    '63247': '&#3597;', #ญ\n",
    "    '61482': '&#42;',   #*\n",
    "    '61591': '&#8226', #•\n",
    "    '63240': '&#3657', # Mai Tho\n",
    "    '63241': '&#3652', #Thantakat (on Po Pla)\n",
    "    '65288': '&#40;', #(\n",
    "    '65289': '&#41;', #)\n",
    "    '61623': '&#8226', #•\n",
    "    '65309': '&#61', #=\n",
    "    '65293': '&#45', #-\n",
    "    '65374': '&#126', #~\n",
    "}\n",
    "\n",
    "def thaiPUA(matchobj):\n",
    "    return pua[matchobj.group(1)]\n",
    "\n",
    "# p = re.compile(r'\\&\\#(\\d{5,})\\;')\n",
    "p = re.compile(r'\\&\\#(6\\d{4,})\\;')\n",
    "\n",
    "for num in range(len(list_of_name)):\n",
    "    output_after_mapping = \"./html_output/after_mapping/\"\n",
    "    os.makedirs(output_after_mapping, exist_ok=True)\n",
    "    outputf = open(f'{output_after_mapping}/{list_of_name[num]}.html', 'w')\n",
    "    inputf = open(f'{output_path}/{list_of_name[num]}.html', 'r')\n",
    "    print(list_of_name[num])\n",
    "    for line in inputf:\n",
    "        text = p.sub(thaiPUA, line)\n",
    "        \n",
    "        outputf.writelines(html.unescape(text))\n",
    "    inputf.close()\n",
    "    outputf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert HTML to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "digits = {\n",
    "    '๐': '0',\n",
    "    '๑': '1',\n",
    "    '๒': '2',\n",
    "    '๓': '3',\n",
    "    '๔': '4',\n",
    "    '๕': '5',\n",
    "    '๖': '6',\n",
    "    '๗': '7',\n",
    "    '๘': '8',\n",
    "    '๙': '9'\n",
    "}\n",
    "\n",
    "def thaiDigits(matchobj):\n",
    "    return digits[matchobj.group(0)]\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def tonemarkPos(matchobj):\n",
    "    return matchobj.group(2) + matchobj.group(1)\n",
    "\n",
    "def tonemarkSpace(matchobj):\n",
    "    return matchobj.group(1) + matchobj.group(2)\n",
    "\n",
    "def process_text(text):\n",
    "    p2 = re.compile(r'([่้๊๋])([ัุูิีึื])') # <tonemark><vowel> -> <vowel><tonemark>\n",
    "    p3 = re.compile(r'([ัุูิีึื])\\s+([่้๊๋])') # <vowel><space><tonemark> -> <vowel><tonemark>\n",
    "    p4 = re.compile(r'[ ]+$', re.MULTILINE) # <space>+ -> <space>\n",
    "    p7 = re.compile(r'\\s+(?=[ัุูิีึื])') # <space><vowel> -> <vowel>\n",
    "\n",
    "    text = strip_tags(text)  # strip html tags\n",
    "    text = p2.sub(tonemarkPos, text)\n",
    "    text = p3.sub(tonemarkSpace, text)\n",
    "    text = p4.sub(thaiDigits, text)\n",
    "    text = p4.sub('', text)\n",
    "    text = p7.sub('', text)\n",
    "    return text\n",
    "\n",
    "def extract_pages(html_content):\n",
    "    pages = re.split(r'<div style=\"page-break-before:always; page-break-after:always\">', html_content)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(len(list_of_name)):\n",
    "\n",
    "    inputf = open(f'./html_output/after_mapping/{list_of_name[num]}.html', 'r')\n",
    "    text = inputf.read()\n",
    "    inputf.close()\n",
    "\n",
    "    text = process_text(text)\n",
    "\n",
    "\n",
    "    outputf = open(f'raw_txt_output/{list_of_name[num]}.txt', 'w')\n",
    "    outputf.write(text)\n",
    "    outputf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift tone with permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shift tone pupu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tone = [3655, 3656, 3657, 3658, 3659, 3660]\n",
    "vowel = [3636, 3637, 3638, 3639, 3640, 3641, 3633]\n",
    "\n",
    "\n",
    "def check_tone_vowel_sentence(new):\n",
    "    last_check = thaispellcheck.check(new)\n",
    "    #print(last_check)\n",
    "    if \"คำผิด\" in last_check:\n",
    "        wrong_word = re.findall(r'<คำผิด>(.*?)</คำผิด>', last_check)\n",
    "        #print(wrong_word)\n",
    "        for word in wrong_word:\n",
    "            word_unicode = [ord(char) for char in word]\n",
    "            corrected = False\n",
    "            for i in range(len(word_unicode)):\n",
    "                if word_unicode[i] in tone or word_unicode[i] in vowel:\n",
    "                    for j in range(i):\n",
    "                        if word_unicode[j] not in tone and word_unicode[j] not in vowel:\n",
    "                            word_unicode[i], word_unicode[j] = word_unicode[j], word_unicode[i]\n",
    "                                # เช็คคำที่แก้ไขแล้วกับ thaispellcheck\n",
    "                            new_word = ''.join(chr(c) for c in word_unicode)\n",
    "                           # print(new_word)\n",
    "                                # print(\"PYTHAI:\", new_word)\n",
    "                            check_result = thaispellcheck.check(new_word)\n",
    "                            if \"คำผิด\" not in check_result:\n",
    "                                corrected = True\n",
    "                                break\n",
    "                            else:\n",
    "                                    # ย้ายกลับตำแหน่งเดิม\n",
    "                                word_unicode[i], word_unicode[j] = word_unicode[j], word_unicode[i]\n",
    "                    if corrected:\n",
    "                        break\n",
    "\n",
    "            if corrected:\n",
    "                new_word = ''.join(chr(c) for c in word_unicode)\n",
    "                new = new.replace(word, new_word)\n",
    "    return new                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shift tone pKo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tone_vowel_word(word):\n",
    "    tone = [chr(3655), chr(3656), chr(3657), chr(3658), chr(3659), chr(3660)]\n",
    "    vowel = [chr(3633), chr(3636), chr(3637), chr(3638), chr(3639), chr(3640), chr(3641)]\n",
    "\n",
    "    def is_tone_or_vowel(char):\n",
    "        return char in tone or char in vowel\n",
    "\n",
    "    # Function to move each marker to all possible positions\n",
    "    def move_markers(word, marker_indices):\n",
    "        permutations = set()\n",
    "\n",
    "        def swap(word_list, i, j):\n",
    "            word_list[i], word_list[j] = word_list[j], word_list[i]\n",
    "            return word_list\n",
    "\n",
    "        # Create all permutations by moving each marker to every possible position\n",
    "        for i, idx in enumerate(marker_indices):\n",
    "            for pos in range(1,len(word)):\n",
    "                if pos != idx and not is_tone_or_vowel(word[pos]):\n",
    "                    new_word = list(word)\n",
    "                    new_word.insert(pos, new_word.pop(idx))\n",
    "                    if \"คำผิด\" not in thaispellcheck.check(\"\".join(new_word)):\n",
    "                        permutations.add(\"\".join(new_word))\n",
    "                        return permutations\n",
    "        permutations.add(\"\")\n",
    "        return permutations\n",
    "    # Find the indices of vowels and tone markers\n",
    "    marker_indices = [i for i, char in enumerate(word) if is_tone_or_vowel(char)]\n",
    "\n",
    "    # Generate permutations by moving markers\n",
    "    all_permutations = set()\n",
    "    all_permutations.update(move_markers(word, marker_indices))\n",
    "    if list(all_permutations)[0] == \"\":\n",
    "        return word\n",
    "    else:\n",
    "        return list(all_permutations)[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_txt_output/10-ก.พ.-2566+Academic_100223_115125.txt',\n",
       " 'raw_txt_output/11-ธ.ค.-2555+Academic_111212_150945.txt',\n",
       " 'raw_txt_output/11-ธ.ค.-2555+Academic_111212_152818.txt',\n",
       " 'raw_txt_output/11-ธ.ค.-2555+Academic_111212_154616.txt',\n",
       " 'raw_txt_output/11-ธ.ค.-2555+Academic_111212_154855.txt',\n",
       " 'raw_txt_output/3-ก.พ.-2565+Academic_110821_091216.txt',\n",
       " 'raw_txt_output/3-ต.ค.-2566+Academic_031023_105516.txt',\n",
       " 'raw_txt_output/4-มิ.ย.-2556+Academic_040613_100424.txt',\n",
       " 'raw_txt_output/4-มิ.ย.-2556+Academic_040613_100623.txt',\n",
       " 'raw_txt_output/9-ส.ค.-2566+Academic_130623_110407.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_path = 'raw_txt_output/'\n",
    "lst_txt = [txt_path+file for file in os.listdir(txt_path)]\n",
    "lst_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\site-packages\\attacut\\models\\__init__.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count wrong word in raw_txt_output/10-ก.พ.-2566+Academic_100223_115125.txt : 1844\n",
      "Count wrong word in raw_txt_output/11-ธ.ค.-2555+Academic_111212_150945.txt : 2\n",
      "Count wrong word in raw_txt_output/11-ธ.ค.-2555+Academic_111212_152818.txt : 283\n",
      "Count wrong word in raw_txt_output/11-ธ.ค.-2555+Academic_111212_154616.txt : 0\n",
      "Count wrong word in raw_txt_output/11-ธ.ค.-2555+Academic_111212_154855.txt : 3925\n",
      "Count wrong word in raw_txt_output/3-ก.พ.-2565+Academic_110821_091216.txt : 383\n",
      "Count wrong word in raw_txt_output/3-ต.ค.-2566+Academic_031023_105516.txt : 1808\n",
      "Count wrong word in raw_txt_output/4-มิ.ย.-2556+Academic_040613_100424.txt : 3191\n",
      "Count wrong word in raw_txt_output/4-มิ.ย.-2556+Academic_040613_100623.txt : 2222\n",
      "Count wrong word in raw_txt_output/9-ส.ค.-2566+Academic_130623_110407.txt : 1604\n"
     ]
    }
   ],
   "source": [
    "txt_path = 'raw_txt_output/'\n",
    "lst_txt = [txt_path+file for file in os.listdir(txt_path)]\n",
    "\n",
    "wrong_words = []\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "for txt_file in lst_txt:\n",
    "\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        txt = f.read()\n",
    "    x = 0\n",
    "    for line in txt.split(\"\\n\"):\n",
    "        fix_text_line = \"\"\n",
    "        if line.strip() != \"\":\n",
    "            new = line.replace(\"ำ\", \"า\")\n",
    "            if re.search(r'([ก-ฮ])ํ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])ํ า', r'\\1ำ', new)\n",
    "            if re.search(r'([ก-ฮ])่ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])่ า', r'\\1่า', new)\n",
    "            if re.search(r'([ก-ฮ])้ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])้ า', r'\\1้า', new)\n",
    "            if re.search(r'([ก-ฮ])๊ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])๊ า', r'\\1๊า', new)\n",
    "            if re.search(r'([ก-ฮ])๋ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])๋ า', r'\\1๋า', new)\n",
    "            if re.search(r'([ก-ฮ]) ำ', new):\n",
    "                new = re.sub(r'([ก-ฮ]) ำ', r'\\1ำ', new)\n",
    "            if re.search(r'([ก-ฮ])า ง', new):\n",
    "                new = re.sub(r'([ก-ฮ])า ง', r'า\\1ง', new)\n",
    "            if re.search(r'([ก-ฮ])า ง', new):\n",
    "                new = re.sub(r'([ก-ฮ])า ง', r'า\\1ง', new)\n",
    "            if re.search(r'([ก-ฮ])ื อ', new):\n",
    "                new = re.sub(r'([ก-ฮ])ื อ', r'\\1ือ', new)\n",
    "            if re.search(r'([ก-ฮ])\\s้', new):\n",
    "                new = re.sub(r'([ก-ฮ])\\s้', r'\\1้', new)\n",
    "            if re.search(r'([ก-ฮ])\\s่', new):\n",
    "                new = re.sub(r'([ก-ฮ])\\s่', r'\\1่', new)\n",
    "            new = new.replace(\" า\", \"ำ\")\n",
    "            new = new.replace(\"่ื\", \"ื่\")\n",
    "            new = new.replace(\"้ื\", \"ื้\")\n",
    "            new = check_tone_vowel_sentence(new)\n",
    "            new_line = tokenize(new)\n",
    "            fix_line = []\n",
    "            for text in new_line:\n",
    "                # print(f'before:{text}')\n",
    "                if \"คำผิด\" in thaispellcheck.check(text):\n",
    "                    text = check_tone_vowel_word(text)\n",
    "                    fix_line.append(text)\n",
    "                    # print(f'after:{text}')\n",
    "                    if \"คำผิด\" in thaispellcheck.check(text):\n",
    "                        x+=1\n",
    "                else:\n",
    "                    fix_line.append(text)\n",
    "            fix_text_line = \"\".join(fix_line)\n",
    "        else:\n",
    "            continue\n",
    "        all_lines.append(fix_text_line)\n",
    "    print(f\"Count wrong word in {txt_file} :\", x)\n",
    "    all_text_lines = \"\\n\".join(all_lines)\n",
    "\n",
    "    with open(f'./corrected_txt_output/{txt_file.split(\"/\")[-1]}', 'w', encoding='utf-8') as f:\n",
    "        f.write(all_text_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----------------------Mapping HTML----------------------------\n",
    "\"\"\"\n",
    "\n",
    "PUA = {\n",
    "    '63233': '&#3636;', # 0xf701 Sara I\n",
    "    '63234': '&#3637;', # 0xf702\n",
    "    '63235': '&#3638;', # 0xf703\n",
    "    '63236': '&#3639;', # 0xf704\n",
    "    '63237': '&#3656;', # 0xf705\n",
    "    '63238': '&#3657;', # 0xf706 Mai Tho (on Po Pla)\n",
    "    '63242': '&#3656;', # 0xf70a Mai Ek\n",
    "    '63243': '&#3657;', # 0xf70b Mai Tho\n",
    "    '63246': '&#3660;', # 0xf70e Thantakat\n",
    "    '63248': '&#3633;', # 0xf710  Mai Han Akhat (on Po Pla)\n",
    "    '63250': '&#3655;', # 0xf712 Mai Tai Khu (on Po Pla)\n",
    "    '63251': '&#3656;', # 0xf713\n",
    "    '63252': '&#3657;', # 0xf714\n",
    "    '63244': '&#3658;', # ๊\n",
    "    '63247': '&#3597;', #ญ\n",
    "    '61482': '&#42;',   #*\n",
    "    '61591': '&#8226', #•\n",
    "    '63240': '&#3657', # Mai Tho\n",
    "    '63241': '&#3652', #Thantakat (on Po Pla)\n",
    "    '65288': '&#40;', #(\n",
    "    '65289': '&#41;', #)\n",
    "    '61623': '&#8226', #•\n",
    "    '65309': '&#61', #=\n",
    "    '65293': '&#45', #-\n",
    "    '65374': '&#126', #~\n",
    "}\n",
    "\n",
    "def thaiPUA(matchobj):\n",
    "    return PUA[matchobj.group(1)]\n",
    "\n",
    "p = re.compile(r'\\&\\#(6\\d{4,})\\;')\n",
    "\n",
    "\"\"\"\n",
    "----------------------HTML to TXT----------------------------\n",
    "\"\"\"\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "    \n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def tonemarkPos(matchobj):\n",
    "    return matchobj.group(2) + matchobj.group(1)\n",
    "\n",
    "def tonemarkSpace(matchobj):\n",
    "    return matchobj.group(1) + matchobj.group(2)\n",
    "\n",
    "def process_text(text):\n",
    "    p2 = re.compile(r'([่้๊๋])([ัุูิีึื])') # <tonemark><vowel> -> <vowel><tonemark>\n",
    "    p3 = re.compile(r'([ัุูิีึื])\\s+([่้๊๋])') # <vowel><space><tonemark> -> <vowel><tonemark>\n",
    "    p4 = re.compile(r'[ ]+$', re.MULTILINE) # <space>+ -> <space>\n",
    "    p7 = re.compile(r'\\s+(?=[ัุูิีึื])') # <space><vowel> -> <vowel>\n",
    "\n",
    "    text = strip_tags(text)  # strip html tags\n",
    "    text = p2.sub(tonemarkPos, text)\n",
    "    text = p3.sub(tonemarkSpace, text)\n",
    "    text = p4.sub('', text)\n",
    "    text = p7.sub('', text)\n",
    "    return text\n",
    "\n",
    "def check_tone_vowel_sentence(new):\n",
    "    tone = [3655, 3656, 3657, 3658, 3659, 3660]\n",
    "    vowel = [3636, 3637, 3638, 3639, 3640, 3641, 3633]\n",
    "    last_check = thaispellcheck.check(new)\n",
    "    #print(last_check)\n",
    "    if \"คำผิด\" in last_check:\n",
    "        wrong_word = re.findall(r'<คำผิด>(.*?)</คำผิด>', last_check)\n",
    "        #print(wrong_word)\n",
    "        for word in wrong_word:\n",
    "            word_unicode = [ord(char) for char in word]\n",
    "            corrected = False\n",
    "            for i in range(len(word_unicode)):\n",
    "                if word_unicode[i] in tone or word_unicode[i] in vowel:\n",
    "                    for j in range(i):\n",
    "                        if word_unicode[j] not in tone and word_unicode[j] not in vowel:\n",
    "                            word_unicode[i], word_unicode[j] = word_unicode[j], word_unicode[i]\n",
    "                                # เช็คคำที่แก้ไขแล้วกับ thaispellcheck\n",
    "                            new_word = ''.join(chr(c) for c in word_unicode)\n",
    "                           # print(new_word)\n",
    "                                # print(\"PYTHAI:\", new_word)\n",
    "                            check_result = thaispellcheck.check(new_word)\n",
    "                            if \"คำผิด\" not in check_result:\n",
    "                                corrected = True\n",
    "                                break\n",
    "                            else:\n",
    "                                    # ย้ายกลับตำแหน่งเดิม\n",
    "                                word_unicode[i], word_unicode[j] = word_unicode[j], word_unicode[i]\n",
    "                    if corrected:\n",
    "                        break\n",
    "\n",
    "            if corrected:\n",
    "                new_word = ''.join(chr(c) for c in word_unicode)\n",
    "                new = new.replace(word, new_word)\n",
    "    return new\n",
    "\n",
    "def check_tone_vowel_word(word):\n",
    "    tone = [chr(3655), chr(3656), chr(3657), chr(3658), chr(3659), chr(3660)]\n",
    "    vowel = [chr(3633), chr(3636), chr(3637), chr(3638), chr(3639), chr(3640), chr(3641)]\n",
    "\n",
    "    def is_tone_or_vowel(char):\n",
    "        return char in tone or char in vowel\n",
    "\n",
    "    # Function to move each marker to all possible positions\n",
    "    def move_markers(word, marker_indices):\n",
    "        permutations = set()\n",
    "\n",
    "        def swap(word_list, i, j):\n",
    "            word_list[i], word_list[j] = word_list[j], word_list[i]\n",
    "            return word_list\n",
    "\n",
    "        # Create all permutations by moving each marker to every possible position\n",
    "        for i, idx in enumerate(marker_indices):\n",
    "            for pos in range(1,len(word)):\n",
    "                if pos != idx and not is_tone_or_vowel(word[pos]):\n",
    "                    new_word = list(word)\n",
    "                    new_word.insert(pos, new_word.pop(idx))\n",
    "                    if \"คำผิด\" not in thaispellcheck.check(\"\".join(new_word)):\n",
    "                        permutations.add(\"\".join(new_word))\n",
    "                        return permutations\n",
    "        permutations.add(\"\")\n",
    "        return permutations\n",
    "    # Find the indices of vowels and tone markers\n",
    "    marker_indices = [i for i, char in enumerate(word) if is_tone_or_vowel(char)]\n",
    "\n",
    "    # Generate permutations by moving markers\n",
    "    all_permutations = set()\n",
    "    all_permutations.update(move_markers(word, marker_indices))\n",
    "    if list(all_permutations)[0] == \"\":\n",
    "        return word\n",
    "    else:\n",
    "        return list(all_permutations)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_name):\n",
    "    \"\"\"\n",
    "        Convert PDF to HTML\n",
    "    \"\"\"\n",
    "    input_format = '--input='+'pdf_example/'+pdf_name\n",
    "    output_path = \"./html_output/before_mapping\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    command = ['java', '-jar', 'pdfbox-app-3.0.2.jar', 'export:text', '-html', input_format, f'--output={output_path}/{pdf_name[:-4]}.html']\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    \"\"\"\n",
    "        Mapping PUA to Unicode\n",
    "    \"\"\"\n",
    "    output_after_mapping = \"./html_output/after_mapping/\"\n",
    "    os.makedirs(output_after_mapping, exist_ok=True)\n",
    "    outputf = open(f'{output_after_mapping}/{pdf_name[:-4]}.html', 'w')\n",
    "    inputf = open(f'{output_path}/{pdf_name[:-4]}.html', 'r')\n",
    "    for line in inputf:\n",
    "        text = p.sub(thaiPUA, line)\n",
    "        outputf.writelines(html.unescape(text))\n",
    "    inputf.close()\n",
    "    outputf.close()\n",
    "\n",
    "    \"\"\"\n",
    "        Extract text from HTML\n",
    "    \"\"\"\n",
    "    inputf = open(f'{output_after_mapping}/{pdf_name[:-4]}.html', 'r')\n",
    "    text = inputf.read()\n",
    "    inputf.close()\n",
    "\n",
    "    text = process_text(text)\n",
    "\n",
    "    os.makedirs('./raw_txt_output', exist_ok=True)\n",
    "    outputf = open(f'raw_txt_output/{pdf_name[:-4]}.txt', 'w')\n",
    "    outputf.write(text)\n",
    "    outputf.close()\n",
    "\n",
    "    \"\"\"\n",
    "        Check tone and vowel\n",
    "    \"\"\"\n",
    "    all_lines = []\n",
    "    x = 0\n",
    "    for line in text.split(\"\\n\"):\n",
    "        fix_text_line = \"\"\n",
    "        if line.strip() != \"\":\n",
    "            new = line.replace(\"ำ\", \"า\")\n",
    "            if re.search(r'([ก-ฮ])ํ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])ํ า', r'\\1ำ', new)\n",
    "            if re.search(r'([ก-ฮ])่ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])่ า', r'\\1่า', new)\n",
    "            if re.search(r'([ก-ฮ])้ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])้ า', r'\\1้า', new)\n",
    "            if re.search(r'([ก-ฮ])๊ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])๊ า', r'\\1๊า', new)\n",
    "            if re.search(r'([ก-ฮ])๋ า', new):\n",
    "                new = re.sub(r'([ก-ฮ])๋ า', r'\\1๋า', new)\n",
    "            if re.search(r'([ก-ฮ]) ำ', new):\n",
    "                new = re.sub(r'([ก-ฮ]) ำ', r'\\1ำ', new)\n",
    "            if re.search(r'([ก-ฮ])า ง', new):\n",
    "                new = re.sub(r'([ก-ฮ])า ง', r'า\\1ง', new)\n",
    "            if re.search(r'([ก-ฮ])า ง', new):\n",
    "                new = re.sub(r'([ก-ฮ])า ง', r'า\\1ง', new)\n",
    "            if re.search(r'([ก-ฮ])ื อ', new):\n",
    "                new = re.sub(r'([ก-ฮ])ื อ', r'\\1ือ', new)\n",
    "            if re.search(r'([ก-ฮ])\\s้', new):\n",
    "                new = re.sub(r'([ก-ฮ])\\s้', r'\\1้', new)\n",
    "            if re.search(r'([ก-ฮ])\\s่', new):\n",
    "                new = re.sub(r'([ก-ฮ])\\s่', r'\\1่', new)\n",
    "            new = new.replace(\" า\", \"ำ\")\n",
    "            new = new.replace(\"่ื\", \"ื่\")\n",
    "            new = new.replace(\"้ื\", \"ื้\")\n",
    "            new = check_tone_vowel_sentence(new)\n",
    "            new_line = tokenize(new)\n",
    "            fix_line = []\n",
    "            for text in new_line:\n",
    "                    # print(f'before:{text}')\n",
    "                    if \"คำผิด\" in thaispellcheck.check(text):\n",
    "                        text = check_tone_vowel_word(text)\n",
    "                        fix_line.append(text)\n",
    "                        # print(f'after:{text}')\n",
    "                        if \"คำผิด\" in thaispellcheck.check(text):\n",
    "                            x+=1\n",
    "                    else:\n",
    "                        fix_line.append(text)\n",
    "            fix_text_line = \"\".join(fix_line)\n",
    "        else:\n",
    "            continue\n",
    "        all_lines.append(fix_text_line)\n",
    "    print(f\"{pdf_name} : Count wrong word :\", x)\n",
    "    all_text_lines = \"\\n\".join(all_lines)\n",
    "\n",
    "    os.makedirs('./corrected_txt_output', exist_ok=True)\n",
    "    with open(f'./corrected_txt_output/{pdf_name[:-4]}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(all_text_lines)\n",
    "    return {pdf_name: x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\site-packages\\attacut\\models\\__init__.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9-ส.ค.-2566+Academic_130623_110407.pdf : Count wrong word : 1604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'9-ส.ค.-2566+Academic_130623_110407.pdf': 1604}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_pdf('9-ส.ค.-2566+Academic_130623_110407.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-ธ.ค.-2555+Academic_111212_154616.pdf : Count wrong word : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\site-packages\\attacut\\models\\__init__.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-ธ.ค.-2555+Academic_111212_152818.pdf : Count wrong word : 283\n",
      "3-ก.พ.-2565+Academic_110821_091216.pdf : Count wrong word : 383\n",
      "11-ธ.ค.-2555+Academic_111212_150945.pdf : Count wrong word : 2\n",
      "11-ธ.ค.-2555+Academic_111212_154855.pdf : Count wrong word : 3925\n",
      "4-มิ.ย.-2556+Academic_040613_100623.pdf : Count wrong word : 2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-มิ.ย.-2556+Academic_040613_100424.pdf : Count wrong word : 3191\n",
      "10-ก.พ.-2566+Academic_100223_115125.pdf : Count wrong word : 1844\n",
      "3-ต.ค.-2566+Academic_031023_105516.pdf : Count wrong word : 1808\n",
      "Error processing PDF: '65288'\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "pdf_list = os.listdir('./pdf_example')\n",
    "\n",
    "# Use ThreadPoolExecutor to process PDFs concurrently\n",
    "max_workers = min(4, len(pdf_list))  # Use at most 4 threads or the number of PDFs\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_pdf, pdf_name) for pdf_name in pdf_list]\n",
    "\n",
    "    # Optionally, wait for all tasks to complete and handle exceptions\n",
    "    for future in futures:\n",
    "        try:\n",
    "            future.result()  # Wait for each task to complete\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing PDF: A process in the process pool was terminated abruptly while the future was running or pending.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "pdf_list = os.listdir('./pdf_example')\n",
    "\n",
    "# Use ProcessPoolExecutor to process PDFs concurrently\n",
    "max_workers = min(4, len(pdf_list))  # Use at most 4 processes or the number of PDFs\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_pdf, pdf_name) for pdf_name in pdf_list]\n",
    "\n",
    "    # Optionally, wait for all tasks to complete and handle exceptions\n",
    "    for future in futures:\n",
    "        try:\n",
    "            future.result()  # Wait for each task to complete\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread in process each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    fix_text_line = \"\"\n",
    "    x = 0\n",
    "    if line.strip() != \"\":\n",
    "        new = line.replace(\"ำ\", \"า\")\n",
    "        if re.search(r'([ก-ฮ])ํ า', new):\n",
    "            new = re.sub(r'([ก-ฮ])ํ า', r'\\1ำ', new)\n",
    "        if re.search(r'([ก-ฮ])่ า', new):\n",
    "            new = re.sub(r'([ก-ฮ])่ า', r'\\1่า', new)\n",
    "        if re.search(r'([ก-ฮ])้ า', new):\n",
    "            new = re.sub(r'([ก-ฮ])้ า', r'\\1้า', new)\n",
    "        if re.search(r'([ก-ฮ])๊ า', new):\n",
    "            new = re.sub(r'([ก-ฮ])๊ า', r'\\1๊า', new)\n",
    "        if re.search(r'([ก-ฮ])๋ า', new):\n",
    "            new = re.sub(r'([ก-ฮ])๋ า', r'\\1๋า', new)\n",
    "        if re.search(r'([ก-ฮ]) ำ', new):\n",
    "            new = re.sub(r'([ก-ฮ]) ำ', r'\\1ำ', new)\n",
    "        if re.search(r'([ก-ฮ])า ง', new):\n",
    "            new = re.sub(r'([ก-ฮ])า ง', r'า\\1ง', new)\n",
    "        if re.search(r'([ก-ฮ])ื อ', new):\n",
    "            new = re.sub(r'([ก-ฮ])ื อ', r'\\1ือ', new)\n",
    "        if re.search(r'([ก-ฮ])\\s้', new):\n",
    "            new = re.sub(r'([ก-ฮ])\\s้', r'\\1้', new)\n",
    "        if re.search(r'([ก-ฮ])\\s่', new):\n",
    "            new = re.sub(r'([ก-ฮ])\\s่', r'\\1่', new)\n",
    "        new = new.replace(\" า\", \"ำ\")\n",
    "        new = new.replace(\"่ื\", \"ื่\")\n",
    "        new = new.replace(\"้ื\", \"ื้\")\n",
    "        new = check_tone_vowel_sentence(new)\n",
    "        new_line = tokenize(new)\n",
    "        fix_line = []\n",
    "        for text in new_line:\n",
    "            if \"คำผิด\" in thaispellcheck.check(text):\n",
    "                text = check_tone_vowel_word(text)\n",
    "                fix_line.append(text)\n",
    "                if \"คำผิด\" in thaispellcheck.check(text):\n",
    "                    x += 1\n",
    "            else:\n",
    "                fix_line.append(text)\n",
    "        fix_text_line = \"\".join(fix_line)\n",
    "    return fix_text_line, x\n",
    "\n",
    "def process_all_text(text):\n",
    "    all_lines = []\n",
    "    total_wrong_words = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Process lines in parallel\n",
    "        results = list(executor.map(process_line, text.split(\"\\n\")))\n",
    "\n",
    "    # Collect results\n",
    "    for fix_text_line, wrong_count in results:\n",
    "        all_lines.append(fix_text_line)\n",
    "        total_wrong_words += wrong_count\n",
    "\n",
    "    print(f\"Count of wrong words: {total_wrong_words}\")\n",
    "    all_text_lines = \"\\n\".join(all_lines)\n",
    "    return all_text_lines, total_wrong_words\n",
    "\n",
    "def process_pdf(pdf_name):\n",
    "    \"\"\"\n",
    "        Convert PDF to HTML\n",
    "    \"\"\"\n",
    "    input_format = '--input='+'pdf_example/'+pdf_name\n",
    "    output_path = \"./html_output/before_mapping\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    command = ['java', '-jar', 'pdfbox-app-3.0.2.jar', 'export:text', '-html', input_format, f'--output={output_path}/{pdf_name[:-4]}.html']\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    \"\"\"\n",
    "        Mapping PUA to Unicode\n",
    "    \"\"\"\n",
    "    output_after_mapping = \"./html_output/after_mapping/\"\n",
    "    os.makedirs(output_after_mapping, exist_ok=True)\n",
    "    outputf = open(f'{output_after_mapping}/{pdf_name[:-4]}.html', 'w')\n",
    "    inputf = open(f'{output_path}/{pdf_name[:-4]}.html', 'r')\n",
    "    for line in inputf:\n",
    "        text = p.sub(thaiPUA, line)\n",
    "        outputf.writelines(html.unescape(text))\n",
    "    inputf.close()\n",
    "    outputf.close()\n",
    "\n",
    "    \"\"\"\n",
    "        Extract text from HTML\n",
    "    \"\"\"\n",
    "    inputf = open(f'{output_after_mapping}/{pdf_name[:-4]}.html', 'r')\n",
    "    text = inputf.read()\n",
    "    inputf.close()\n",
    "\n",
    "    text = process_text(text)\n",
    "\n",
    "    os.makedirs('./raw_txt_output', exist_ok=True)\n",
    "    outputf = open(f'raw_txt_output/{pdf_name[:-4]}.txt', 'w')\n",
    "    outputf.write(text)\n",
    "    outputf.close()\n",
    "\n",
    "    \"\"\"\n",
    "        Check tone and vowel\n",
    "    \"\"\"\n",
    "    all_text_lines, x = process_all_text(text)\n",
    "\n",
    "    os.makedirs('./corrected_txt_output', exist_ok=True)\n",
    "    with open(f'./corrected_txt_output/{pdf_name[:-4]}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(all_text_lines)\n",
    "    return pdf_name, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\site-packages\\attacut\\models\\__init__.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of wrong words: 1844\n",
      "10-ก.พ.-2566+Academic_100223_115125.pdf : Count wrong word : 1844\n",
      "Count of wrong words: 2\n",
      "11-ธ.ค.-2555+Academic_111212_150945.pdf : Count wrong word : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of wrong words: 283\n",
      "11-ธ.ค.-2555+Academic_111212_152818.pdf : Count wrong word : 283\n",
      "Count of wrong words: 0\n",
      "11-ธ.ค.-2555+Academic_111212_154616.pdf : Count wrong word : 0\n",
      "Count of wrong words: 3925\n",
      "11-ธ.ค.-2555+Academic_111212_154855.pdf : Count wrong word : 3925\n",
      "Count of wrong words: 383\n",
      "3-ก.พ.-2565+Academic_110821_091216.pdf : Count wrong word : 383\n",
      "Count of wrong words: 1808\n",
      "3-ต.ค.-2566+Academic_031023_105516.pdf : Count wrong word : 1808\n",
      "Count of wrong words: 3191\n",
      "4-มิ.ย.-2556+Academic_040613_100424.pdf : Count wrong word : 3191\n",
      "Count of wrong words: 2222\n",
      "4-มิ.ย.-2556+Academic_040613_100623.pdf : Count wrong word : 2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\subprocess.py\", line 1495, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"c:\\Users\\Lenovo\\anaconda3\\envs\\openth\\lib\\codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'65288'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pdf_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pdf_example\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_name \u001b[38;5;129;01min\u001b[39;00m pdf_list:\n\u001b[1;32m----> 3\u001b[0m     result, x \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : Count wrong word : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 78\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[1;34m(pdf_name)\u001b[0m\n\u001b[0;32m     76\u001b[0m inputf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m inputf:\n\u001b[1;32m---> 78\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthaiPUA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     outputf\u001b[38;5;241m.\u001b[39mwritelines(html\u001b[38;5;241m.\u001b[39munescape(text))\n\u001b[0;32m     80\u001b[0m inputf\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mthaiPUA\u001b[1;34m(matchobj)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthaiPUA\u001b[39m(matchobj):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPUA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmatchobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: '65288'"
     ]
    }
   ],
   "source": [
    "pdf_list = os.listdir('./pdf_example')\n",
    "for pdf_name in pdf_list:\n",
    "    result, x = process_pdf(pdf_name)\n",
    "    print(f\"{pdf_name} : Count wrong word : {x}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
